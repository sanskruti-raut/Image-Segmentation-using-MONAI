{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbee11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.data import Dataset, DataLoader\n",
    "from functools import partial\n",
    "from monai.data import (\n",
    "    ThreadDataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    set_track_meta,\n",
    ")\n",
    "\n",
    "import torch\n",
    "# Set CUDA device order\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Check MONAI version\n",
    "print_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197397a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/scratch1/ajoshi/mac_bse_data/data\"\n",
    "img_add = os.path.join(data_dir, \"sub-032100_ses-003_run-1_T1w.nii.gz\")\n",
    "label_add = os.path.join(data_dir, \"sub-032100_ses-003_run-1_T1w_mask.nii.gz\")\n",
    "\n",
    "\n",
    "img = nib.load(img_add).get_fdata()\n",
    "label = nib.load(label_add).get_fdata()\n",
    "\n",
    "print(f\"image shape: {img.shape}, label shape: {label.shape}\")\n",
    "\n",
    "plt.figure(\"image\", (18, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(img[:, :, 78], cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:, :, 78])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "def datafold_read(data_dir, val_split=0.2):\n",
    "    # Get a list of all files in the data directory\n",
    "    files = glob(data_dir+'/*T1w.nii.gz')\n",
    "    #print(files)\n",
    "    random.shuffle(files)  # Shuffle the list to ensure randomness\n",
    "\n",
    "    # Calculate the number of validation files based on the validation split percentage\n",
    "    num_val_files = int(val_split * len(files))\n",
    "\n",
    "    # Split the files into training and validation sets\n",
    "    train_files = files[num_val_files:]\n",
    "    val_files = files[:num_val_files]\n",
    "\n",
    "    # Create dictionaries containing image and label paths for training and validation sets\n",
    "    train_data = [{\"image\": file, \"label\": file.replace('.nii.gz','_mask.nii.gz')} for file in train_files]\n",
    "    val_data = [{\"image\": file, \"label\": file.replace('.nii.gz','_mask.nii.gz')} for file in val_files]\n",
    "\n",
    "    return train_files, val_files, train_data, val_data\n",
    "\n",
    "# Define path to your data directory\n",
    "data_dir = \"/scratch1/ajoshi/mac_bse_data/data\"\n",
    "\n",
    "# Call the datafold_read function to split the data into training and validation sets\n",
    "train_files, val_files, train_data, val_data = datafold_read(data_dir)\n",
    "\n",
    "# Print the first few entries from the training and validation files lists\n",
    "print(\"Training files:\")\n",
    "for file_info in train_files[:5]:\n",
    "    print(file_info)\n",
    "\n",
    "print(\"\\nValidation files:\")\n",
    "for file_info in val_files[:5]:\n",
    "    print(file_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6399f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# AverageMeter class\n",
    "class AverageMeter(object):\n",
    "    def __init__(self, num_classes=1):\n",
    "        self.reset(num_classes)\n",
    "\n",
    "    def reset(self, num_classes):\n",
    "        self.val = [0] * num_classes\n",
    "        self.avg = [0] * num_classes\n",
    "        self.sum = [0] * num_classes\n",
    "        self.count = [0] * num_classes\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if isinstance(val, (list, tuple)):\n",
    "            for i in range(len(val)):\n",
    "                self.val[i] = val[i]\n",
    "                self.sum[i] += val[i] * n\n",
    "                self.count[i] += n\n",
    "                self.avg[i] = np.where(self.count[i] > 0, self.sum[i] / self.count[i], self.sum[i])\n",
    "        else:\n",
    "            self.val[0] = val\n",
    "            self.sum[0] += val * n\n",
    "            self.count[0] += n\n",
    "            self.avg[0] = np.where(self.count[0] > 0, self.sum[0] / self.count[0], self.sum[0])\n",
    "\n",
    "# Save checkpoint function\n",
    "def save_checkpoint(model, epoch, optimizer, filename=\"model.pt\", best_acc=0, dir_add=\".\"):\n",
    "    state_dict = {\n",
    "        \"epoch\": epoch,\n",
    "        \"best_acc\": best_acc,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    filename = os.path.join(dir_add, filename)\n",
    "    torch.save(state_dict, filename)\n",
    "    print(\"Saving checkpoint:\", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba743e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.transforms import Compose, LoadImaged, ConvertToMultiChannelBasedOnBratsClassesd, CropForegroundd, RandSpatialCropd, RandFlipd, NormalizeIntensityd, RandScaleIntensityd, RandShiftIntensityd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_loader(batch_size, data_dir, roi):\n",
    "\n",
    "    # Get a list of all files in the data directory\n",
    "    all_files = glob(data_dir+'/*T1w.nii.gz')\n",
    "    #print(files)\n",
    "    random.shuffle(all_files)  # Shuffle the list to ensure randomness    \n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    train_files, val_files = train_test_split(all_files, test_size=0.2, random_state=42)\n",
    "    \n",
    "  \n",
    "\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            transforms.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "\n",
    "            transforms.Resized(\n",
    "                keys=[\"image\",\"label\"],\n",
    "                spatial_size=roi,\n",
    "                mode={\"trilinear\",\"nearest\"},\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    val_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            transforms.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "            transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create dictionaries containing image and label paths for training and validation sets\n",
    "    train_data = [{\"image\": file, \"label\": file.replace('.nii.gz','_mask.nii.gz')} for file in train_files]\n",
    "    val_data = [{\"image\": file, \"label\": file.replace('.nii.gz','_mask.nii.gz')} for file in val_files]\n",
    "\n",
    "    \n",
    "    train_ds = Dataset(data=train_data, transform=train_transform)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    val_ds = Dataset(data=val_data, transform=val_transform)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Define path to your data directory\n",
    "data_dir = \"/scratch1/ajoshi/mac_bse_data/data\"\n",
    "\n",
    "roi = (128, 128, 128)\n",
    "batch_size = 4\n",
    "sw_batch_size = 4\n",
    "infer_overlap = 0.5\n",
    "max_epochs = 10\n",
    "val_every = 10\n",
    "train_loader, val_loader = get_loader(batch_size, data_dir, roi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3de6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_loader)\n",
    "# print(len(train_files))\n",
    "\n",
    "# batch = next(iter(train_loader))\n",
    "# print(batch[\"image\"].shape)\n",
    "\n",
    "\n",
    "#train_data = [{\"image\": os.path.join(data_dir, file), \"label\": os.path.join(data_dir, file)} for file in train_files]\n",
    "#print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed55827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_loader)\n",
    "# print(len(val_files))\n",
    "\n",
    "# batch = next(iter(val_loader))\n",
    "# print(batch[\"image\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a03b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, batch_data in enumerate(train_loader):\n",
    "    \n",
    "#     data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "#     print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf52bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, batch_data in enumerate(val_loader):\n",
    "    \n",
    "#     data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "#     print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d243cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.utils.misc import ensure_tuple_rep\n",
    "\n",
    "# Assuming `input_image` is your input image with dimensions (H, W, D)\n",
    "input_image = np.random.randn(176, 176, 96)  # Example input image\n",
    "\n",
    "# Determine the target dimensions that are divisible by 32\n",
    "target_h = int(np.ceil(input_image.shape[0] / 32) * 32)\n",
    "target_w = int(np.ceil(input_image.shape[1] / 32) * 32)\n",
    "target_d = int(np.ceil(input_image.shape[2] / 32) * 32)\n",
    "\n",
    "# Calculate the amount of padding required on each side\n",
    "pad_h = max(0, target_h - input_image.shape[0])\n",
    "pad_w = max(0, target_w - input_image.shape[1])\n",
    "pad_d = max(0, target_d - input_image.shape[2])\n",
    "\n",
    "# Calculate the padding values for each side\n",
    "pad_top = pad_h // 2\n",
    "pad_bottom = pad_h - pad_top\n",
    "pad_left = pad_w // 2\n",
    "pad_right = pad_w - pad_left\n",
    "pad_front = pad_d // 2\n",
    "pad_back = pad_d - pad_front\n",
    "\n",
    "# Pad the input image using numpy.pad\n",
    "padded_image = np.pad(input_image, ((pad_top, pad_bottom), (pad_left, pad_right), (pad_front, pad_back)), mode='constant')\n",
    "\n",
    "print(\"Original input image shape:\", input_image.shape)\n",
    "print(\"Padded input image shape:\", padded_image.shape)\n",
    "\n",
    "# Set CUDA device order\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model parameters\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "feature_size = 48\n",
    "drop_rate = 0.0\n",
    "attn_drop_rate = 0.0\n",
    "dropout_path_rate = 0.0\n",
    "\n",
    "# Create the SwinUNETR model with the padded input image\n",
    "roi = padded_image.shape\n",
    "model = SwinUNETR(\n",
    "    img_size=roi,\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    feature_size=feature_size,\n",
    "    drop_rate=drop_rate,\n",
    "    attn_drop_rate=attn_drop_rate,\n",
    "    dropout_path_rate=dropout_path_rate,\n",
    "    spatial_dims=3,  # Assuming 3D data\n",
    "    #downsample=True,  # This will downsample the input to match patch size\n",
    "    use_checkpoint=True,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "dice_loss = DiceLoss(to_onehot_y=False, sigmoid=True)\n",
    "post_sigmoid = Activations(sigmoid=True)\n",
    "post_pred = AsDiscrete(argmax=False, threshold=0.5)\n",
    "dice_acc = DiceMetric(include_background=True, reduction=MetricReduction.MEAN_BATCH, get_not_nans=True)\n",
    "model_inferer = partial(\n",
    "    sliding_window_inference,\n",
    "    roi_size=[roi[0], roi[1], roi[2]],\n",
    "    sw_batch_size=sw_batch_size,\n",
    "    predictor=model,\n",
    "    overlap=infer_overlap,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OG CODE\n",
    "\n",
    "def train_epoch(model, loader, optimizer, epoch, loss_func):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    run_loss = AverageMeter()\n",
    "    for idx, batch_data in enumerate(loader):\n",
    "        data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "        logits = model(data)\n",
    "        loss = loss_func(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        run_loss.update(loss.item(), n=batch_size)\n",
    "        print(\n",
    "            \"Epoch {}/{} {}/{}\".format(epoch, max_epochs, idx, len(loader)),\n",
    "            \"loss: {:.4f}\".format(float(run_loss.avg[0])),\n",
    "            \"time {:.2f}s\".format(time.time() - start_time),\n",
    "        )\n",
    "        start_time = time.time()\n",
    "    return run_loss.avg\n",
    "\n",
    "\n",
    "def val_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    epoch,\n",
    "    acc_func,\n",
    "    model_inferer=None,\n",
    "    post_sigmoid=None,\n",
    "    post_pred=None,\n",
    "):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    run_acc = AverageMeter(num_classes=1)  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch_data in enumerate(loader):\n",
    "            data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "            logits = model_inferer(data)\n",
    "            val_labels_list = decollate_batch(target)\n",
    "            val_outputs_list = decollate_batch(logits)\n",
    "            val_output_convert = [post_pred(post_sigmoid(val_pred_tensor)) for val_pred_tensor in val_outputs_list]\n",
    "            acc_func.reset()\n",
    "            acc_func(y_pred=val_output_convert, y=val_labels_list)\n",
    "            acc, not_nans = acc_func.aggregate()\n",
    "            run_acc.update(acc.cpu().numpy(), n=not_nans.cpu().numpy())\n",
    "\n",
    "            # # Check if all elements in run_acc.avg have the same shape or type\n",
    "            # if not all(isinstance(x, type(run_acc.avg[0])) for x in run_acc.avg):\n",
    "            #     print(f\"Warning: Inconsistent element types in run_acc.avg: {run_acc.avg}\")\n",
    "            #     continue  # Skip this iteration\n",
    "\n",
    "            # Convert scalar values to arrays for consistency\n",
    "            dice_tc = np.array([run_acc.avg[0]]) if not isinstance(run_acc.avg[0], np.ndarray) else run_acc.avg[0]\n",
    "            # dice_wt = np.array([run_acc.avg[1]]) if not isinstance(run_acc.avg[1], np.ndarray) else run_acc.avg[1]\n",
    "            # dice_et = np.array([run_acc.avg[2]]) if not isinstance(run_acc.avg[2], np.ndarray) else run_acc.avg[2]\n",
    "\n",
    "\n",
    "            print(\n",
    "                \"Val {}/{} {}/{}\".format(epoch, max_epochs, idx, len(loader)),\n",
    "                \", dice_tc:\",\n",
    "                dice_tc,\n",
    "                # \", dice_wt:\",\n",
    "                # dice_wt,\n",
    "                # \", dice_et:\",\n",
    "                # dice_et,\n",
    "                \", time {:.2f}s\".format(time.time() - start_time),\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "    return run_acc.avg    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff54031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OG CODE\n",
    "\n",
    "def trainer(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    loss_func,\n",
    "    acc_func,\n",
    "    scheduler,\n",
    "    model_inferer=None,\n",
    "    start_epoch=0,\n",
    "    post_sigmoid=None,\n",
    "    post_pred=None,\n",
    "):\n",
    "    val_acc_max = 0.0\n",
    "    dices_tc = []\n",
    "    dices_wt = []\n",
    "    dices_et = []\n",
    "    dices_avg = []\n",
    "    loss_epochs = []\n",
    "    trains_epoch = []\n",
    "    for epoch in range(start_epoch, max_epochs):\n",
    "        print(time.ctime(), \"Epoch:\", epoch)\n",
    "        epoch_time = time.time()\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            epoch=epoch,\n",
    "            loss_func=loss_func,\n",
    "        )\n",
    "        print(\n",
    "            \"Final training  {}/{}\".format(epoch, max_epochs - 1),\n",
    "            \"loss: {:.4f}\".format(train_loss[0]),  # Format the first element of the list\n",
    "            \"time {:.2f}s\".format(time.time() - epoch_time),\n",
    "        )\n",
    "\n",
    "        if (epoch + 1) % val_every == 0 or epoch == 0:\n",
    "            loss_epochs.append(train_loss[0])  # Append the first element of the list\n",
    "            trains_epoch.append(int(epoch))\n",
    "            epoch_time = time.time()\n",
    "            val_acc = val_epoch(\n",
    "                model,\n",
    "                val_loader,\n",
    "                epoch=epoch,\n",
    "                acc_func=acc_func,\n",
    "                model_inferer=model_inferer,\n",
    "                post_sigmoid=post_sigmoid,\n",
    "                post_pred=post_pred,\n",
    "            )\n",
    "            dice_tc = val_acc[0]\n",
    "            # dice_wt = val_acc[1]\n",
    "            # dice_et = val_acc[2]\n",
    "            val_avg_acc = np.mean(val_acc) \n",
    "            # val_avg_acc = np.mean([dice_tc, dice_wt, dice_et])\n",
    "            print(\n",
    "                \"Final validation stats {}/{}\".format(epoch, max_epochs - 1),\n",
    "                \", dice_tc:\",\n",
    "                dice_tc,\n",
    "                # \", dice_wt:\",\n",
    "                # dice_wt,\n",
    "                # \", dice_et:\",\n",
    "                # dice_et,\n",
    "                \", Dice_Avg:\",\n",
    "                val_avg_acc,\n",
    "                \", time {:.2f}s\".format(time.time() - epoch_time),\n",
    "            )\n",
    "            dices_tc.append(dice_tc)\n",
    "            # dices_wt.append(dice_wt)\n",
    "            # dices_et.append(dice_et)\n",
    "            dices_avg.append(val_avg_acc)\n",
    "            if val_avg_acc > val_acc_max:\n",
    "                print(\"new best ({:.6f} --> {:.6f}). \".format(val_acc_max, val_avg_acc))\n",
    "                val_acc_max = val_avg_acc\n",
    "                save_checkpoint(\n",
    "                    model,\n",
    "                    epoch,\n",
    "                    optimizer,\n",
    "                    best_acc=val_acc_max,\n",
    "                )\n",
    "            scheduler.step()\n",
    "    print(\"Training Finished !, Best Accuracy: \", val_acc_max)\n",
    "    return (\n",
    "        val_acc_max,\n",
    "        dices_tc,\n",
    "        dices_wt,\n",
    "        dices_et,\n",
    "        dices_avg,\n",
    "        loss_epochs,\n",
    "        trains_epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "(\n",
    "    val_acc_max,\n",
    "    dices_tc,\n",
    "    dices_wt,\n",
    "    dices_et,\n",
    "    dices_avg,\n",
    "    loss_epochs,\n",
    "    trains_epoch,\n",
    ") = trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=dice_loss,\n",
    "    acc_func=dice_acc,\n",
    "    scheduler=scheduler,\n",
    "    model_inferer=model_inferer,\n",
    "    start_epoch=start_epoch,\n",
    "    post_sigmoid=post_sigmoid,\n",
    "    post_pred=post_pred,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746514e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb91fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e38063e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f71fb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571a6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f65b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
